# IA Multimodal

Los modelos multimodales procesan y generan mÃºltiples tipos de datos: texto, imÃ¡genes, audio y video. Representan la convergencia de la IA hacia sistemas que perciben el mundo como los humanos.

## Â¿QuÃ© es la IA multimodal?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MODELO MULTIMODAL                  â”‚
â”‚                                                 â”‚
â”‚  ENTRADAS:              SALIDAS:                â”‚
â”‚  ğŸ“ Texto        â”€â”€â”   â”Œâ”€â”€â–¶  ğŸ“ Texto          â”‚
â”‚  ğŸ–¼ï¸ ImÃ¡genes     â”€â”€â”¤   â”‚                       â”‚
â”‚  ğŸ”Š Audio        â”€â”€â”¼â”€â”€â–¶â”œâ”€â”€â–¶  ğŸ–¼ï¸ ImÃ¡genes       â”‚
â”‚  ğŸ¬ Video        â”€â”€â”¤   â”‚                       â”‚
â”‚  ğŸ“Š Datos        â”€â”€â”˜   â””â”€â”€â–¶  ğŸ”Š Audio          â”‚
â”‚                                                 â”‚
â”‚  Ejemplos:                                      â”‚
â”‚  - GPT-4o: texto + imagen + audio â†’ texto       â”‚
â”‚  - Gemini 2.0: texto + imagen + video â†’ texto   â”‚
â”‚  - DALL-E 3: texto â†’ imagen                     â”‚
â”‚  - Sora: texto â†’ video                          â”‚
â”‚  - Whisper: audio â†’ texto                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## VisiÃ³n: anÃ¡lisis de imÃ¡genes

### Con GPT-4o

```python
from openai import OpenAI
import base64

client = OpenAI()

# OpciÃ³n 1: imagen desde URL
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "Â¿QuÃ© hay en esta imagen? Describe con detalle."},
            {"type": "image_url", "image_url": {
                "url": "https://example.com/foto.jpg",
                "detail": "high"  # "low", "high", o "auto"
            }}
        ]
    }],
    max_tokens=500,
)

print(response.choices[0].message.content)

# OpciÃ³n 2: imagen en base64
with open("diagram.png", "rb") as f:
    base64_image = base64.b64encode(f.read()).decode()

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "Explica este diagrama de arquitectura."},
            {"type": "image_url", "image_url": {
                "url": f"data:image/png;base64,{base64_image}"
            }}
        ]
    }],
)
```

### Casos de uso de vision

```python
# OCR avanzado: extraer texto de documentos
def extract_text_from_document(image_path):
    with open(image_path, "rb") as f:
        b64 = base64.b64encode(f.read()).decode()

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{
            "role": "user",
            "content": [
                {"type": "text", "text": """Extrae TODO el texto de esta imagen.
                MantÃ©n el formato y estructura original.
                Devuelve solo el texto extraÃ­do."""},
                {"type": "image_url", "image_url": {
                    "url": f"data:image/png;base64,{b64}"
                }}
            ]
        }]
    )
    return response.choices[0].message.content

# AnÃ¡lisis de grÃ¡ficos y charts
def analyze_chart(chart_image_path):
    """Analiza un grÃ¡fico y extrae insights."""
    # GPT-4o puede leer barras, lÃ­neas, pie charts, etc.
    response = extract_from_image(
        chart_image_path,
        "Analiza este grÃ¡fico. Â¿CuÃ¡les son las tendencias principales? "
        "Â¿QuÃ© datos especÃ­ficos puedes extraer?"
    )
    return response
```

## GeneraciÃ³n de imÃ¡genes

### DALL-E 3

```python
response = client.images.generate(
    model="dall-e-3",
    prompt="Un robot programador trabajando en una laptop, estilo acuarela",
    size="1024x1024",
    quality="hd",
    n=1,
)

image_url = response.data[0].url
revised_prompt = response.data[0].revised_prompt  # DALL-E mejora tu prompt
print(f"Prompt usado: {revised_prompt}")
```

### Stable Diffusion (open source)

```python
from diffusers import StableDiffusionXLPipeline
import torch

pipe = StableDiffusionXLPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16,
)
pipe = pipe.to("cuda")

image = pipe(
    prompt="A futuristic city at sunset, cyberpunk style, 4k",
    negative_prompt="blurry, low quality, distorted",
    num_inference_steps=30,
    guidance_scale=7.5,
).images[0]

image.save("city.png")
```

## Audio: Speech-to-Text y Text-to-Speech

### Whisper (transcripciÃ³n)

```python
# Whisper de OpenAI: transcripciÃ³n multilingÃ¼e
audio_file = open("podcast.mp3", "rb")

transcript = client.audio.transcriptions.create(
    model="whisper-1",
    file=audio_file,
    language="es",
    response_format="verbose_json",
    timestamp_granularities=["segment"],
)

for segment in transcript.segments:
    print(f"[{segment['start']:.1f}s] {segment['text']}")

# Whisper local (open source)
import whisper

model = whisper.load_model("large-v3")
result = model.transcribe("audio.mp3", language="es")
print(result["text"])
```

### Text-to-Speech

```python
# OpenAI TTS
response = client.audio.speech.create(
    model="tts-1-hd",
    voice="nova",        # alloy, echo, fable, onyx, nova, shimmer
    input="Hola, soy una voz generada por inteligencia artificial.",
    speed=1.0,
)

response.stream_to_file("output.mp3")

# ElevenLabs (alta calidad, clonaciÃ³n de voz)
from elevenlabs import generate, play

audio = generate(
    text="Este es un ejemplo de voz clonada.",
    voice="tu_voz_clonada",
    model="eleven_multilingual_v2",
)
play(audio)
```

## Video: GeneraciÃ³n y anÃ¡lisis

```python
# AnÃ¡lisis de video con Gemini 2.0
import google.generativeai as genai

model = genai.GenerativeModel("gemini-2.0-flash")

# Subir video
video_file = genai.upload_file("demo.mp4")

# Analizar
response = model.generate_content([
    video_file,
    "Describe lo que ocurre en este video paso a paso."
])
print(response.text)

# GeneraciÃ³n de video (Sora y similares)
# OpenAI Sora: genera videos de hasta 60s desde texto
# RunwayML Gen-3: genera clips de 10s
# Nota: estas APIs son de acceso limitado (2025)
```

## Modelos multimodales nativos

```python
# GPT-4o: nativo multimodal (texto + imagen + audio)
# Una sola llamada puede procesar mÃºltiples modalidades

response = client.chat.completions.create(
    model="gpt-4o-audio-preview",
    modalities=["text", "audio"],
    audio={"voice": "alloy", "format": "wav"},
    messages=[{
        "role": "user",
        "content": "Explica quÃ© es la fotosÃ­ntesis en una oraciÃ³n."
    }]
)

# Respuesta en texto Y audio simultÃ¡neamente
print(response.choices[0].message.content)       # Texto
audio_data = response.choices[0].message.audio    # Audio WAV

# Gemini 2.0: procesa texto, images, audio, video, cÃ³digo
# en una sola API unificada
```

## Resumen

- Los modelos multimodales procesan texto, imÃ¡genes, audio y video.
- GPT-4o y Gemini 2.0 son nativamente multimodales.
- Vision: OCR, anÃ¡lisis de diagramas, descripciÃ³n de imÃ¡genes.
- Audio: Whisper para transcripciÃ³n, TTS para generaciÃ³n de voz.
- GeneraciÃ³n de imÃ¡genes: DALL-E 3, Stable Diffusion (open source).
- Video: anÃ¡lisis con Gemini, generaciÃ³n temprana con Sora.
- La tendencia es hacia modelos unificados que manejan todas las modalidades.
